---
title: "An introduction to GPHS"
output: pdf_document
header-includes:
  \usepackage{fvextra}
  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
vignette: >
  %\VignetteIndexEntry{An introduction to GPHS}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
\tableofcontents
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
## Introduction
Hyperparameter selection in bayesian inference is very crucial as it specifies the covariance structure. Typically, we integrate over these hyperparameters but they often correspond to high-dimensional integrals which becomes analytically intractable. This integration is often performed by  MCMC, but MCMC converges very slowly and it needs expert tuning. Hence, we need special techniques to handle these situations. \
`GPHS` is an R package that provides various methods to perform hyperparameter selection in Gaussian processes. It has three algorithms to perform the update of hyperparameters. We have Metropolis-Hastings algorithm and the Slice sampling algorithm for a Surrogate data model as discussed in Murray and Adams (2010)\footnote{Murray and Adams (2010)}. We also have another function that implements the Elliptical Slice Sampling as given in Murray et al (2010) and Nishihara, Murray and Adams (2014). In addition to these algorithms, `GPHS` also has a function to invert symmetric matrices based on Cholesky decomposition, which is faster than the default `solve()` function in R.\

### Elliptical Slice Sampling (ESS)
Elliptical Slice Sampling, first discussed by Murray and Adams in 2010, is a MCMC technique that provides a way to sample along an ellipse with an adaptive step-size. It is designed to sample from posterior of the form $\pi(x)\propto L(x)N(x;\mu,\Sigma)$, where $L(\cdot)$ denotes the likelihood function and $N(\mu,\Sigma)$ denotes a multivariate Gaussian prior. The major advantage of this method is that it has no free tuning parameters. It relies on the fact that Gaussian prior mixes rapidly even when there is strong dependency induced by the covariance structure.\
ESS uses the invariance property that when $x$ and $\nu$ are drawn independently from $N(\mu,\Sigma)$ distribution then the linear combination: 
$$x'=(x-\mu)\cos(\theta) + (\nu-\mu)\sin(\theta)$$ 
is marginally distributed as $N(\mu,\Sigma)$ for any $\theta\in[0,2\pi]$, although $x'$ is still correlated with $x$ and $\nu$.

### Surrogate Data model:
As discussed in Murray and Adams(2010), we consider the following generic form of the model:
$$\begin{aligned}
\text{covariance hyperparameters } & \theta\propto p_h\\
\text{latent variable } & f\propto N(0,\Sigma_{\theta})\\
\text{conditional likelihood }& P(data|f)=L(f)
\end{aligned}
$$
We assume that the covariance matrix $\Sigma_\theta$ is arbitrary positive definite matrix. For non-Gaussian likelihoods, we sample from the joint posterior:
$$P(f,\theta)\propto L(f)N(f;0,\Sigma_\theta)p_h(\theta)$$
To guide joint proporsals of the hyperparameters and the latent variables, we create an auxiliary variable that introduces a surrogate gaussian observation. The Gaussian model is augmented with a noisy version of the true latent variable, $g$:
$$P(g|f,\theta)=N(g;f,S_\theta)$$
Here, $S_\theta$ can be set by hand to a fixed value or a value depending on the current hyperparameters. We first draw the auxiliary variable from the marginal distribution:
$$P(g|\theta)=N(g;0,\Sigma_\theta+S_\theta)$$
and then sample the latent values conditioned on the auxiliary variables:
$$P(f|g,\theta)=N(f;m_{\theta,g},R_\theta),\text{ where } R_{\theta}=\left(\Sigma_\theta^{-1}+S_\theta^{-1}\right)^{-1}\text{ and }m_{\theta,g}=R_\theta S_\theta^{-1}g$$
The sampling process can be described like a draw from a spheical Gaussian as follows:
$$\eta\sim N(0,I),\; f=L_{R_\theta}\eta+m_{\theta,g},\text{ where }L_{R_\theta}L_{R_\theta}^T=R_\theta$$
We update the hyperparameters $\theta$  conditioned on the 
"whitened" variables $\eta$ and the surrogate data $g$.
$$P(\theta|\eta,g,data)\propto P(\theta,\eta,g,data)\propto L(f(\theta,\eta,g))N(g;0,\Sigma_\theta+S_\theta)p_h(\theta)$$.

The acceptance rule in the Metropolis-Hastings algorithm contains a ratio of the above. We implement this algorithm in the function `SurrogateMH`.\

However, the major problem with the Metropolis-Hastings algorithm is that the proposal distribution needs to be set and tuned properly. The efficiency of the algorithms depend on the scale $\sigma$ of the proposal distribution. Slice sampling, a family of adaptive procedures, is much more robust to the choice of scale parameter. We implement a possible type of slice sampling in the function `SurrogateSS`.

## Installation

You can install the most recent version of 'GPHS' package from [GitHub](https://github.com/niladrik/GPHS) using the following commands:
```{r, eval = FALSE}
# Plain installation
devtools::install_github("niladrik/GPHS")

# For installation with vignette
devtools::install_github("niladrik/GPHS", build_vignettes = TRUE)
```

## Quick Start
This section will convey a general idea of the package. We would briefly mention the functions that are there in the package, their inputs and their outputs.\
Firstly, we load the package.
```{r setup}
library(GPHS)
```
Next we create a dataset that we are going to use to depict the functionality of our functions:
```{r, warning=FALSE}
# loading ggplot for plots
library(ggplot2)
# setting the seed for reproducibility of the data
set.seed(12345)

f_data <- function(x){
  func <- 0
  for(j in 1:5000){
    func <- func + j^(-1.7) * sin(j) * cos(pi * (j - 0.5) * x)
  }
  return(func)
}

x = seq(1, 1e-3, -0.01)
# true value (latent variable)
ff = f_data(x)

# observed value is anoisy version of the true value
y = ff + rnorm(length(x), 0, sd(ff)/10)
```
Thus, the function and the data points we generated look like:

```{r}
  ggplot(data = as.data.frame(cbind(x, y, ff)), aes(x = x, y = y)) + geom_point() + geom_line(aes(x = x, y = ff, color = "red")) + theme_bw() + theme(legend.position = "none")
```

For the purpose of illustration, we consider a sample of 100 data points from this set as our data in hand.
```{r}
# selecting a sample of 100 data points
x.train = sample(x = x, size = 100, replace = FALSE)
y.train = sample(x = y, size = 100, replace = FALSE)

# dataset in hand
training_data = cbind(x.train, y.train)
```

